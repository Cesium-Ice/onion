{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # PART 1: GETTING DATA\n",
    " Data is extracted from Reddit using the Pushshift API as documented here:  \n",
    " &ensp;https://github.com/pushshift/api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting dates to extract data between\n",
    "The Pushshift API only returns at most 1000 posts with each request, so I create a list of dates to pull ~1000 posts between these dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01/01/2010',\n",
       " '01/06/2010',\n",
       " '01/01/2011',\n",
       " '01/06/2011',\n",
       " '01/01/2012',\n",
       " '01/06/2012',\n",
       " '01/01/2013',\n",
       " '01/06/2013',\n",
       " '01/01/2014',\n",
       " '01/06/2014',\n",
       " '01/01/2015',\n",
       " '01/06/2015',\n",
       " '01/01/2016',\n",
       " '01/06/2016',\n",
       " '01/01/2017',\n",
       " '01/06/2017',\n",
       " '01/01/2018',\n",
       " '01/06/2018',\n",
       " '01/01/2019',\n",
       " '01/06/2019',\n",
       " '01/01/2020']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_list = []\n",
    "\n",
    "# Making list of dates; Each January 1st and June 1st from 2015 until January 1st 2020\n",
    "for i in range(10,21):\n",
    "    dates_list.append('01/01/20'+str(i))\n",
    "    dates_list.append('01/06/20'+str(i))\n",
    "\n",
    "# Popping June 2020 since it hasn't happened yet\n",
    "dates_list.pop()\n",
    "dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting dates to Unix timestamp\n",
    "Returns Unix timestamp that Pushshift API requires for dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1262332800,\n",
       " 1275375600,\n",
       " 1293868800,\n",
       " 1306911600,\n",
       " 1325404800,\n",
       " 1338534000,\n",
       " 1357027200,\n",
       " 1370070000,\n",
       " 1388563200,\n",
       " 1401606000,\n",
       " 1420099200,\n",
       " 1433142000,\n",
       " 1451635200,\n",
       " 1464764400,\n",
       " 1483257600,\n",
       " 1496300400,\n",
       " 1514793600,\n",
       " 1527836400,\n",
       " 1546329600,\n",
       " 1559372400,\n",
       " 1577865600]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTimeStamp(date_input):\n",
    "    return time.mktime(datetime.datetime.strptime(date_input, \"%d/%m/%Y\").timetuple())\n",
    "\n",
    "dates = [int(getTimeStamp(date)) for date in dates_list]\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Pushshift data\n",
    "Returns the top 1000 posts in the given subreddit between the given times.  \n",
    "Code modified from the following article:  \n",
    "&ensp;https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(after, before, sub):\n",
    "    url = ('https://api.pushshift.io/reddit/search/submission/?size=1000&after='+\n",
    "           str(after)+'&before='+str(before)+'&subreddit='+str(sub)+'&sort_type=score'+'&sort=desc')\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the titles between the dates chosen earlier\n",
    "Here I loop through all the dates above and get the top ~1000 posts in the chosen subreddit.   \n",
    "\n",
    "I end up with 9065 Onion headlines and 15432 \"fake\"-Onion headlines from r/NotThenOnion.  \n",
    "I then keep the first 9000 and first 15000 for easier batching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1262332800&before=1275375600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1275375600&before=1293868800&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1293868800&before=1306911600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1306911600&before=1325404800&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1325404800&before=1338534000&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1338534000&before=1357027200&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1357027200&before=1370070000&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1370070000&before=1388563200&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1388563200&before=1401606000&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1401606000&before=1420099200&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1420099200&before=1433142000&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1433142000&before=1451635200&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1451635200&before=1464764400&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1464764400&before=1483257600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1483257600&before=1496300400&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1496300400&before=1514793600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1514793600&before=1527836400&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1527836400&before=1546329600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546329600&before=1559372400&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559372400&before=1577865600&subreddit=nottheonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1262332800&before=1275375600&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1275375600&before=1293868800&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1293868800&before=1306911600&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1306911600&before=1325404800&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1325404800&before=1338534000&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1338534000&before=1357027200&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1357027200&before=1370070000&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1370070000&before=1388563200&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1388563200&before=1401606000&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1401606000&before=1420099200&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1420099200&before=1433142000&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1433142000&before=1451635200&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1451635200&before=1464764400&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1464764400&before=1483257600&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1483257600&before=1496300400&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1496300400&before=1514793600&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1514793600&before=1527836400&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1527836400&before=1546329600&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546329600&before=1559372400&subreddit=theonion&sort_type=score&sort=desc\n",
      "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559372400&before=1577865600&subreddit=theonion&sort_type=score&sort=desc\n"
     ]
    }
   ],
   "source": [
    "def getTitles(subreddit):\n",
    "    titles_new = []\n",
    "    titles = []\n",
    "\n",
    "    for i in range(len(dates)-1):\n",
    "        # Setting up dates\n",
    "        after  = dates[i]\n",
    "        before = dates[i+1]\n",
    "\n",
    "        # Getting subreddit data between the dates after and before from r/NotTheOnion\n",
    "        raw_json = getPushshiftData(after,before,subreddit)\n",
    "\n",
    "        # Extracting just the title\n",
    "        titles_new = [post['title'] for post in raw_json]\n",
    "\n",
    "        # Appending new data on\n",
    "        titles = titles + titles_new\n",
    "\n",
    "    # A few posts were extracted twice, set gets rid of duplicates\n",
    "    titles = list(set(titles))\n",
    "    return titles\n",
    "\n",
    "not_onion = getTitles('nottheonion')\n",
    "onion = getTitles('theonion')\n",
    "\n",
    "onion = onion[:9000]\n",
    "not_onion = not_onion[:15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pandas dataframe\n",
    "Labeling Onion headlines as 1, and r/NotTheOnion headlines as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>entire facebook staff laughs as man tightens p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>muslim woman denied soda can for fear she coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bold move hulu has announced that theyre gonna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>despondent jeff bezos realizes hell have to wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>for men looking for great single women online ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  entire facebook staff laughs as man tightens p...      1\n",
       "1  muslim woman denied soda can for fear she coul...      0\n",
       "2  bold move hulu has announced that theyre gonna...      1\n",
       "3  despondent jeff bezos realizes hell have to wo...      1\n",
       "4  for men looking for great single women online ...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1= pd.DataFrame({'text':onion})\n",
    "df1['label'] = 1\n",
    "\n",
    "df2 = pd.DataFrame({'text':not_onion})\n",
    "df2['label'] = 0\n",
    "\n",
    "# Combining both datasets\n",
    "df = pd.concat([df1,df2])\n",
    "\n",
    "# Shuffling the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Saving the 'uncleaned' dataframe to a csv file\n",
    "df.to_csv('OnionOrNot.csv', index = False)\n",
    "\n",
    "# Converting all text to lowercase, fixing ampersands and getting rid\n",
    "# of dashes and apostrophes as they can mess up the dictionary\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text'] = df['text'].str.replace(r'&amp;', 'and')\n",
    "df['text'] = df['text'].str.replace(r'-', ' ')\n",
    "df['text'] = df['text'].str.replace(r'[^\\s\\w]','')\n",
    "\n",
    "# Saving the dataframe to a csv file\n",
    "df.to_csv('OnionOrNotClean.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in dataframe\n",
    "Running this when I return to the project so I don't have to use the Pushshift API etc. again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>entire facebook staff laughs as man tightens p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>muslim woman denied soda can for fear she coul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bold move hulu has announced that theyre gonna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>despondent jeff bezos realizes hell have to wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>for men looking for great single women online ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  entire facebook staff laughs as man tightens p...      1\n",
       "1  muslim woman denied soda can for fear she coul...      0\n",
       "2  bold move hulu has announced that theyre gonna...      1\n",
       "3  despondent jeff bezos realizes hell have to wo...      1\n",
       "4  for men looking for great single women online ...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('OnionOrNotClean.csv', index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: ENCODING WORDS AS NUMBERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all the words in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "sentence_lengths = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Updates adds all items to the set, re.split splits the text into words\n",
    "    sentence_words = re.split(r'\\s',df.iloc[i]['text'])\n",
    "    vocab_set.update(sentence_words)\n",
    "    sentence_lengths.append(len(sentence_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the words to a dictionary\n",
    "This way we can map the words in the dataframe to lists of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(vocab_set)\n",
    "vocab_dict = {vocab_list[i-1]: i for i in range(1, len(vocab_list)+1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating column of the words mapped to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(sentence_lengths)\n",
    "\n",
    "def toNumbers(row):\n",
    "    words = re.findall(r'([\\w]+)', row['text'])\n",
    "    nums =  np.array([vocab_dict[words[j]] for j in range(len(words))])\n",
    "    return np.pad(nums, (0, max_length - len(nums)), mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [10101, 15701, 24365, 6689, 22221, 4330, 4928,...\n",
       "1    [6556, 7335, 1523, 21250, 6690, 23567, 18468, ...\n",
       "2    [20493, 17894, 4253, 9925, 21346, 24068, 7515,...\n",
       "3    [18219, 15505, 9902, 24892, 16634, 10504, 810,...\n",
       "4    [16068, 3826, 16392, 14837, 1613, 5793, 15082,...\n",
       "Name: nums, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = df.apply(lambda row: toNumbers(row), axis=1) \n",
    "df['nums'] = nums\n",
    "\n",
    "df['nums'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24000, 64), (24000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.asarray(df['label'].values)\n",
    "features = np.stack(df['nums'].values)\n",
    "\n",
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3: BUILDING, COMPILING, AND FITTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/5\n",
      "19200/19200 - 173s - loss: 0.4103 - accuracy: 0.8130 - val_loss: 0.3051 - val_accuracy: 0.8715\n",
      "Epoch 2/5\n",
      "19200/19200 - 152s - loss: 0.1713 - accuracy: 0.9377 - val_loss: 0.3448 - val_accuracy: 0.8652\n",
      "Epoch 3/5\n",
      "19200/19200 - 116s - loss: 0.0685 - accuracy: 0.9778 - val_loss: 0.4366 - val_accuracy: 0.8554\n",
      "Epoch 4/5\n",
      "19200/19200 - 121s - loss: 0.0348 - accuracy: 0.9891 - val_loss: 0.6293 - val_accuracy: 0.8344\n",
      "Epoch 5/5\n",
      "19200/19200 - 127s - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.6790 - val_accuracy: 0.8442\n"
     ]
    }
   ],
   "source": [
    "def get_compiled_model():\n",
    "    embedding_dim=16\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(len(vocab_set)+1, 64),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(features, labels, batch_size=32, epochs=5, verbose=2, validation_split=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning parameters, modifying model, etc.\n",
    "\n",
    "### Trial 1: ~85% validation accuracy around epoch 5\n",
    "```\n",
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(len(vocab_set)+1, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])```\n",
    "\n",
    "### Trial 2: ~85% validation accuracy around epoch 5  \n",
    "No discernable change in accuracy tuning embedding_dim \n",
    "```\n",
    "embedding_dim=32\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(len(vocab_set)+1, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])```\n",
    "\n",
    "### Trail 3: ~87% validation accuracy on epoch 1\n",
    "Starts overfitting after epoch 1\n",
    "```\n",
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab_set)+1, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])```\n",
    "\n",
    "### Trail 4: ~87% validation accuracy on epoch 1\n",
    "Again starts overfitting after epoch 1\n",
    "```\n",
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(vocab_set)+1, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
